{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 최종 모델: Stacked GRU\n",
    "\n",
    "### 1. Model 구성\n",
    "- 베이스라인 모델 참조 Bidrirectional StackedGRU\n",
    "\n",
    "### 2. 최종 모델 성능 최적화 방법\n",
    "1. Model의 ouput에 lowpass filter를 적용해 noise를 제거 (데이터후처리)\n",
    "\n",
    "\n",
    "### 3. 모델 선정 기준\n",
    "1. 일반화 성능이 높은 모델 (Validation data TaPR과 Public score의 차이가 적은 모델)\n",
    "2. Public score가 높은 모델 \n",
    "\n",
    "### 4. 최종 모델 성능\n",
    "- Publice score : 0.60163\n",
    "- Private score : 0.60695\n",
    "\n",
    "### 5. 시도해본 방법들\n",
    "\n",
    "- 모델 관점 \n",
    "    1. LSTM AutoEncoder 모델\n",
    "    2. 1D CNN 모델\n",
    "\n",
    "    \n",
    "   \n",
    "- 데이터 전처리관점\n",
    "    1. feature selection\n",
    " \n",
    "\n",
    "### 6. Path 정보\n",
    "\n",
    "- Data Path\n",
    "    1. traing data : data/train/\n",
    "    2. validation data : data/validation/\n",
    "    3. test data : data/test/\n",
    "    \n",
    "    \n",
    "- Model Path\n",
    "    1. model 저장 경로 : ./answer.pt\n",
    "    2. 최종제출 model : ./answer.pt\n",
    "    \n",
    "    \n",
    "- Submission Path\n",
    "    1. submission 저장 경로 : ./\n",
    "    2. sample path : data/sample_submission.csv\n",
    "    3. 최종제출 submisstion : ./answer.csv\n",
    "    \n",
    "    \n",
    "- TaPR Lib path\n",
    "    1. whl file : data/eTaPR-21.8.2-py3-none-any.whl\n",
    "    \n",
    "### 7. Library 버전\n",
    "- toolz==0.10.0\n",
    "- torch==1.4.0\n",
    "- torchvision==0.5.0\n",
    "- torchviz==0.0.1\n",
    "- tornado==6.0.4\n",
    "- numpy==1.18.5\n",
    "- notebook==6.0.3\n",
    "- pyparsing==2.4.7\n",
    "- python 3\n",
    "- matplotlib \n",
    "- pandas\n",
    "- dateutil\n",
    "- datetime\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install data/eTaPR-21.8.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "import dateutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import trange\n",
    "from TaPR_pkg import etapr\n",
    "\n",
    "#random seed를 사용함으로써 매번 달라지는 training set과 validation set을 고정시킴\n",
    "import random\n",
    "random_seed=1011\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "#window size 설정\n",
    "WINDOW_SIZE = 30\n",
    "WINDOW_GIVEN = WINDOW_SIZE-1\n",
    "\n",
    "idx=\"answer\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* lowpass filter, moving average, range check, gray area 함수를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PvRHLXri9WFH"
   },
   "outputs": [],
   "source": [
    "# 모델의 결과(anomaly score)를 smoothing하기 위해서 다양한 filter를 적용\n",
    "# 데이터 후처리(lowpass필터 적용)를 진행하면 아래와 같이 anomaly score가 더 안정적으로 변함을 확인할 수 있다. (Test Dataset에서는 그 효과가 더 자세히 나타난다.)\n",
    "# the order of the filer : 1\n",
    "# the ciritical frequency of the filter : 0.1\n",
    "from scipy import signal\n",
    "b, a = signal.butter(1, 0.1, btype='lowpass')\n",
    "\n",
    "\n",
    "#Moving average\n",
    "def moving_average(ANOMALY_SCORE, window=10):\n",
    "    ma=[]\n",
    "    for idx in range(len(ANOMALY_SCORE)):\n",
    "        if idx >= window:\n",
    "            ma.append((ANOMALY_SCORE[idx-window:idx].mean()+ANOMALY_SCORE[idx])/2)\n",
    "        else:\n",
    "            ma.append(ANOMALY_SCORE[idx])\n",
    "    ma=np.array(ma)\n",
    "    return ma\n",
    "\n",
    "\n",
    "#Range check\n",
    "def range_check(series, size):\n",
    "    data = []\n",
    "    for i in range(len(series)-size+1):\n",
    "        if i == 0 :\n",
    "            check_std = np.std(series[i:i+size])\n",
    "        std = np.std(series[i:i+size])\n",
    "        mean = np.mean(series[i:i+size])\n",
    "        max = np.max(series[i:i+size])\n",
    "        if check_std * 2 >= std:\n",
    "            data.append(mean)\n",
    "            check_std = std\n",
    "        elif max == series[i]:\n",
    "            data.append(max*5)\n",
    "            check_std = std\n",
    "        else:\n",
    "            data.append(series[i]*3)\n",
    "    for _ in range(size-1):\n",
    "        data.append(mean)\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "#Gray Area(2020 수상팀인 SIILAB의 Gray Aare Smoothing 코드  참조)\n",
    "def Gray_Area(attacks):\n",
    "    start = []  # start point\n",
    "    finish = []  # finish point\n",
    "    c = []  # count\n",
    "    com = 0\n",
    "    count = 0\n",
    "    for i in range(1, len(attacks)):\n",
    "        if attacks[i - 1] != attacks[i]:\n",
    "            if com == 0:\n",
    "                start.append(i)\n",
    "                count = count + 1\n",
    "                com = 1\n",
    "            elif com == 1:\n",
    "                finish.append(i - 1)\n",
    "                c.append(count)\n",
    "                count = 0\n",
    "                start.append(i)\n",
    "                count = count + 1\n",
    "        else:\n",
    "            count = count + 1\n",
    "    finish.append(len(attacks) - 1)\n",
    "    c.append(finish[len(finish) - 1] - start[len(start) - 1] + 1)\n",
    "    for i in range(0, len(start)):\n",
    "        if c[i] < 10:\n",
    "            s = start[i]\n",
    "            f = finish[i] + 1\n",
    "            g1 = [1 for i in range(c[i])] # Temp Attack list\n",
    "            g0 = [0 for i in range(c[i])]  # Temp Normal List\n",
    "            if attacks[start[i] - 1] == 1:\n",
    "                attacks[s:f] = g1  # change to attack\n",
    "            else:\n",
    "                attacks[s:f] = g0  # change to normal\n",
    "    return attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train data, Test data, Validation data 에 대해 csv 데이터를 읽어오고 합치는 과정을 수행합니다. timpestamp 필드만 삭제하고 모델을 학습하였습니다. min-max scaler를 사용하여 feature scaling을 진행하였습니다. 그 후 ewm(alpha=0.9)를 적용해 노이즈를 제거하였습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_from_csv(target):\n",
    "    return pd.read_csv(target).rename(columns=lambda x: x.strip())\n",
    "\n",
    "def dataframe_from_csvs(targets):\n",
    "    return pd.concat([dataframe_from_csv(x) for x in targets])\n",
    "\n",
    "TRAIN_DATASET = sorted([x for x in Path(\"data/train/\").glob(\"*.csv\")])\n",
    "VALIDATION_DATASET = sorted([x for x in Path(\"data/validation/\").glob(\"*.csv\")])\n",
    "TEST_DATASET = sorted([x for x in Path(\"data/test/\").glob(\"*.csv\")])\n",
    "\n",
    "TRAIN_DF_RAW = dataframe_from_csvs(TRAIN_DATASET)\n",
    "VALIDATION_DF_RAW = dataframe_from_csvs(VALIDATION_DATASET)\n",
    "TEST_DF_RAW = dataframe_from_csvs(TEST_DATASET)\n",
    "\n",
    "TIMESTAMP_FIELD = \"timestamp\"\n",
    "ATTACK_FIELD = \"attack\"\n",
    "VALID_COLUMNS_IN_TRAIN_DATASET = TRAIN_DF_RAW.columns.drop([TIMESTAMP_FIELD])\n",
    "\n",
    "TAG_MIN = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].min()\n",
    "TAG_MAX = TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET].max()\n",
    "\n",
    "#Normalization\n",
    "def normalize(df):\n",
    "    ndf = df.copy()\n",
    "    for c in df.columns:\n",
    "        if TAG_MIN[c] == TAG_MAX[c]:\n",
    "            ndf[c] = df[c] - TAG_MIN[c]\n",
    "        else:\n",
    "            ndf[c] = (df[c] - TAG_MIN[c]) / (TAG_MAX[c] - TAG_MIN[c])\n",
    "    return ndf\n",
    "\n",
    "TRAIN_DF = normalize(TRAIN_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
    "VALIDATION_DF = normalize(VALIDATION_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET])\n",
    "TEST_DF = normalize(TEST_DF_RAW[VALID_COLUMNS_IN_TRAIN_DATASET]).ewm(alpha=0.9).mean()\n",
    "\n",
    "#boundary check\n",
    "def boundary_check(df):\n",
    "    x = np.array(df, dtype=np.float32)\n",
    "    return np.any(x > 1.0), np.any(x < 0), np.any(np.isnan(x))\n",
    "\n",
    "boundary_check(TRAIN_DF), boundary_check(VALIDATION_DF), boundary_check(TEST_DF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaiDataset(Dataset):\n",
    "    def __init__(self, timestamps, df, stride=1, attacks=None):\n",
    "        self.ts = np.array(timestamps)\n",
    "        self.tag_values = np.array(df, dtype=np.float32)\n",
    "        self.valid_idxs = []\n",
    "        for L in trange(len(self.ts) - WINDOW_SIZE + 1):\n",
    "            R = L + WINDOW_SIZE - 1\n",
    "            if dateutil.parser.parse(self.ts[R]) - dateutil.parser.parse(\n",
    "                self.ts[L]\n",
    "            ) == timedelta(seconds=WINDOW_SIZE - 1):\n",
    "                self.valid_idxs.append(L)\n",
    "        self.valid_idxs = np.array(self.valid_idxs, dtype=np.int32)[::stride]\n",
    "        self.n_idxs = len(self.valid_idxs)\n",
    "        print(f\"# of valid windows: {self.n_idxs}\")\n",
    "        if attacks is not None:\n",
    "            self.attacks = np.array(attacks, dtype=np.float32)\n",
    "            self.with_attack = True\n",
    "        else:\n",
    "            self.with_attack = False\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_idxs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.valid_idxs[idx]\n",
    "        last = i + WINDOW_SIZE - 1\n",
    "        item = {\"attack\": self.attacks[last]} if self.with_attack else {}\n",
    "        item[\"ts\"] = self.ts[i + WINDOW_SIZE - 1]\n",
    "        item[\"given\"] = torch.from_numpy(self.tag_values[i : i + WINDOW_GIVEN])\n",
    "        item[\"answer\"] = torch.from_numpy(self.tag_values[last])\n",
    "        return item\n",
    "    \n",
    "HAI_DATASET_TRAIN = HaiDataset(TRAIN_DF_RAW[TIMESTAMP_FIELD], TRAIN_DF, stride=1)\n",
    "HAI_DATASET_VALIDATION = HaiDataset(\n",
    "    VALIDATION_DF_RAW[TIMESTAMP_FIELD], VALIDATION_DF, attacks=VALIDATION_DF_RAW[ATTACK_FIELD]\n",
    ")\n",
    "HAI_DATASET_TEST = HaiDataset(\n",
    "    TEST_DF_RAW[TIMESTAMP_FIELD], TEST_DF, attacks=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 모델 하이퍼파라미터 설정입니다.  \n",
    "    * Hidden cells : 200  \n",
    "    * Layer : 3  \n",
    "    * Batch size : 4096  \n",
    "    * Epoch : 200  \n",
    "    * Drop out : 0    \n",
    "    * Learning rate : 1e-3  \n",
    "    * Moving average는 window size의 2배, range check는 30으로 설정하였습니다.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mzqmi7n_Ccp"
   },
   "outputs": [],
   "source": [
    "import easydict\n",
    "args = easydict.EasyDict({\n",
    "                        'N_HIDDENS' : 200,\n",
    "                        'N_LAYERS' : 3,\n",
    "                        'batch_size' : 4096,\n",
    "                        'epochs' : 200,\n",
    "                        'dropout' : 0,\n",
    "                        'lr' : 1e-3,\n",
    "                        'moving_average' : WINDOW_SIZE*2,\n",
    "                        'range_check' : 30\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedGRU(torch.nn.Module):\n",
    "    def __init__(self, n_tags):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.GRU(\n",
    "            input_size=n_tags,\n",
    "            hidden_size=args.N_HIDDENS,\n",
    "            num_layers=args.N_LAYERS,\n",
    "            bidirectional=True,\n",
    "            dropout=args.dropout,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(args.N_HIDDENS * 2, n_tags)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)  # (batch, seq, params) -> (seq, batch, params)\n",
    "        self.rnn.flatten_parameters()\n",
    "        outs, _ = self.rnn(x)\n",
    "        out = self.fc(outs[-1])\n",
    "        return x[0] + out\n",
    "    \n",
    "MODEL = StackedGRU(n_tags=TRAIN_DF.shape[1])\n",
    "MODEL.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def train(dataset, model, batch_size, n_epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    epochs = trange(n_epochs, desc=\"training\")\n",
    "    best = {\"loss\": sys.float_info.max}\n",
    "    loss_history = []\n",
    "    for e in epochs:\n",
    "        epoch_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            given = batch[\"given\"].cuda()\n",
    "            guess = model(given)\n",
    "            answer = batch[\"answer\"].cuda()\n",
    "            loss = loss_fn(answer, guess)\n",
    "            loss.backward()\n",
    "            epoch_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        loss_history.append(epoch_loss)\n",
    "        epochs.set_postfix_str(f\"loss: {epoch_loss:.6f}\")\n",
    "        if epoch_loss < best[\"loss\"]:\n",
    "            best[\"state\"] = model.state_dict()\n",
    "            best[\"loss\"] = epoch_loss\n",
    "            best[\"epoch\"] = e + 1\n",
    "    return best, loss_history\n",
    "\n",
    "MODEL.train()\n",
    "BEST_MODEL, LOSS_HISTORY = train(HAI_DATASET_TRAIN, MODEL, args.batch_size, args.epochs)\n",
    "print(f'best_loss: {BEST_MODEL[\"loss\"]}, best_epoch: {BEST_MODEL[\"epoch\"]}.')\n",
    "\n",
    "with open(idx+'.pt', \"wb\") as f:\n",
    "    torch.save(\n",
    "        {\n",
    "            \"state\": BEST_MODEL[\"state\"],\n",
    "            \"best_epoch\": BEST_MODEL[\"epoch\"],\n",
    "            \"loss_history\": LOSS_HISTORY,\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(idx+'.pt', \"rb\") as f:\n",
    "    SAVED_MODEL = torch.load(f)\n",
    "\n",
    "MODEL.load_state_dict(SAVED_MODEL[\"state\"])\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.title(\"Training Loss Graph\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(SAVED_MODEL[\"loss_history\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(dataset, model, batch_size):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    ts, dist, att = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            given = batch[\"given\"].cuda()\n",
    "            answer = batch[\"answer\"].cuda()\n",
    "            guess = model(given)\n",
    "            ts.append(np.array(batch[\"ts\"]))\n",
    "            dist.append(torch.abs(answer - guess).cpu().numpy())\n",
    "            try:\n",
    "                att.append(np.array(batch[\"attack\"]))\n",
    "            except:\n",
    "                att.append(np.zeros(batch_size))\n",
    "    return (\n",
    "        np.concatenate(ts),\n",
    "        np.concatenate(dist),\n",
    "        np.concatenate(att),\n",
    "    )\n",
    "\n",
    "def check_graph(xs, att, piece=2, THRESHOLD=None):\n",
    "    l = xs.shape[0]\n",
    "    chunk = l // piece\n",
    "    fig, axs = plt.subplots(piece, figsize=(20, 4 * piece))\n",
    "    for i in range(piece):\n",
    "        L = i * chunk\n",
    "        R = min(L + chunk, l)\n",
    "        xticks = range(L, R)\n",
    "        axs[i].plot(xticks, xs[L:R])\n",
    "        if len(xs[L:R]) > 0:\n",
    "            peak = max(xs[L:R])\n",
    "            axs[i].plot(xticks, att[L:R] * peak * 0.3)\n",
    "        if THRESHOLD!=None:\n",
    "            axs[i].axhline(y=THRESHOLD, color='r')\n",
    "    plt.show()\n",
    "\n",
    "def put_labels(distance, threshold):\n",
    "    xs = np.zeros_like(distance)\n",
    "    xs[distance > threshold] = 1\n",
    "    return xs\n",
    "\n",
    "def fill_blank(check_ts, labels, total_ts):\n",
    "    def ts_generator():\n",
    "        for t in total_ts:\n",
    "            yield dateutil.parser.parse(t)\n",
    "    def label_generator():\n",
    "        for t, label in zip(check_ts, labels):\n",
    "            yield dateutil.parser.parse(t), label\n",
    "    g_ts = ts_generator()\n",
    "    g_label = label_generator()\n",
    "    final_labels = []\n",
    "    try:\n",
    "        current = next(g_ts)\n",
    "        ts_label, label = next(g_label)\n",
    "        while True:\n",
    "            if current > ts_label:\n",
    "                ts_label, label = next(g_label)\n",
    "                continue\n",
    "            elif current < ts_label:\n",
    "                final_labels.append(0)\n",
    "                current = next(g_ts)\n",
    "                continue\n",
    "            final_labels.append(label)\n",
    "            current = next(g_ts)\n",
    "            ts_label, label = next(g_label)\n",
    "    except StopIteration:\n",
    "        return np.array(final_labels, dtype=np.int8)\n",
    "\n",
    "    \n",
    "#threshold 설정 --> delta 값을 기준으로 최적의 threshold 값 선정    \n",
    "def opt_thresh(anom_score, check_ts, total_ts, atk_label):\n",
    "    # initial values\n",
    "    tap, tar = 0.0, 1.0     # initial recall and precision\n",
    "    q, d = 0.99, 0.01       # initial quantile and distance\n",
    "    delta = 0.015\n",
    "    # loop until tap and tar are close enough\n",
    "    while abs(tap - tar) > delta:\n",
    "        t, d = np.quantile(anom_score, q), d/2      # update t and d\n",
    "        anom_label = fill_blank(check_ts, put_labels(anom_score, t), total_ts)\n",
    "        tapr = etapr.evaluate_haicon(anomalies=atk_label, predictions=anom_label)\n",
    "        tap, tar = float(tapr['TaP']), float(tapr['TaR'])\n",
    "        if tar > tap:\n",
    "            q = q + d\n",
    "        else:\n",
    "            q = q - d\n",
    "    return t, tapr, anom_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 후처리 및 threshold 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "MODEL.eval()\n",
    "CHECK_TS, CHECK_DIST, CHECK_ATT = inference(HAI_DATASET_VALIDATION, MODEL, args.batch_size)\n",
    "\n",
    "ANOMALY_SCORE = np.mean(CHECK_DIST, axis=1)\n",
    "\n",
    "ANOMALY_SCORE = signal.filtfilt(b,a,ANOMALY_SCORE)\n",
    "ANOMALY_SCORE = moving_average(ANOMALY_SCORE, window=args.moving_average)\n",
    "ANOMALY_SCORE = range_check(ANOMALY_SCORE, size=args.range_check)\n",
    "\n",
    "thr, _, _ = opt_thresh(ANOMALY_SCORE, CHECK_TS, np.array(VALIDATION_DF_RAW[TIMESTAMP_FIELD]), CHECK_ATT)\n",
    "print('threshold:', thr)\n",
    "THRESHOLD = 0.008\n",
    "\n",
    "LABELS = put_labels(ANOMALY_SCORE, THRESHOLD)\n",
    "ATTACK_LABELS = put_labels(np.array(VALIDATION_DF_RAW[ATTACK_FIELD]), threshold=0.5)\n",
    "FINAL_LABELS = fill_blank(CHECK_TS, LABELS, np.array(VALIDATION_DF_RAW[TIMESTAMP_FIELD]))\n",
    "FINAL_LABELS = Gray_Area(LABELS)\n",
    "\n",
    "check_graph(ANOMALY_SCORE, CHECK_ATT, piece=2, THRESHOLD=THRESHOLD)\n",
    "\n",
    "TaPR = etapr.evaluate_haicon(anomalies=ATTACK_LABELS, predictions=FINAL_LABELS)\n",
    "print(f\"F1: {TaPR['f1']:.3f} (TaP: {TaPR['TaP']:.3f}, TaR: {TaPR['TaR']:.3f})\")\n",
    "print(f\"# of detected anomalies: {len(TaPR['Detected_Anomalies'])}\")\n",
    "print(f\"Detected anomalies: {TaPR['Detected_Anomalies']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "MODEL.eval()\n",
    "CHECK_TS, CHECK_DIST, CHECK_ATT = inference(HAI_DATASET_TEST, MODEL, args.batch_size)\n",
    "\n",
    "ANOMALY_SCORE = np.mean(CHECK_DIST, axis=1)\n",
    "\n",
    "ANOMALY_SCORE = signal.filtfilt(b,a,ANOMALY_SCORE)\n",
    "ANOMALY_SCORE = moving_average(ANOMALY_SCORE, window=args.moving_average)\n",
    "ANOMALY_SCORE = range_check(ANOMALY_SCORE, size=args.range_check)\n",
    "\n",
    "LABELS = put_labels(ANOMALY_SCORE, THRESHOLD)\n",
    "\n",
    "check_graph(ANOMALY_SCORE, CHECK_ATT, piece=3, THRESHOLD=THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission.index = submission['timestamp']\n",
    "submission.loc[CHECK_TS,'attack'] = LABELS\n",
    "submission['attack']=Gray_Area(submission['attack'].copy())\n",
    "submission.to_csv(idx+'.csv', index=False)\n",
    "\n",
    "submission['attack'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-9.m80",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-9:m80"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
